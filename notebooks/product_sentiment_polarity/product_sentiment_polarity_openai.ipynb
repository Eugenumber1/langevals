{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the top 3 benefits of Galaxy AI?</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>The top 3 benefits of the Galaxy AI are: unpar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the main differences between the Sams...</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>The main differences between the Samsung Galax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does the Samsung Galaxy S23 Ultra support 8K v...</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>Absolutely! The Samsung Galaxy S23 Ultra does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the battery life of the Galaxy S23+ c...</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>The Galaxy S23+ offers a remarkable battery li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can I use a stylus with the Samsung Galaxy S23...</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>Absolutely! The Samsung Galaxy S23 Ultra is de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question      sentiment  \\\n",
       "0          What are the top 3 benefits of Galaxy AI?  very_positive   \n",
       "1  What are the main differences between the Sams...  very_positive   \n",
       "2  Does the Samsung Galaxy S23 Ultra support 8K v...  very_positive   \n",
       "3  How does the battery life of the Galaxy S23+ c...  very_positive   \n",
       "4  Can I use a stylus with the Samsung Galaxy S23...  very_positive   \n",
       "\n",
       "                                              answer  \n",
       "0  The top 3 benefits of the Galaxy AI are: unpar...  \n",
       "1  The main differences between the Samsung Galax...  \n",
       "2  Absolutely! The Samsung Galaxy S23 Ultra does ...  \n",
       "3  The Galaxy S23+ offers a remarkable battery li...  \n",
       "4  Absolutely! The Samsung Galaxy S23 Ultra is de...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./product_sentiment_polarity/data/samsung-labeled-transformed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import dspy\n",
    "\n",
    "# Load math questions from the GSM8K dataset\n",
    "dataset = []\n",
    "for _, row in df.iterrows():\n",
    "    dataset.append(\n",
    "        dspy.Example(output=row.answer, sentiment=row.sentiment).with_inputs(\"output\")\n",
    "    )\n",
    "trainset, devset = train_test_split(dataset, train_size=50, test_size=0.1)\n",
    "print(\"trainset len\", len(trainset))\n",
    "trainset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there\n",
      "\n",
      "Negative Points: None\n",
      "\n",
      "Sentiment: subtly_positive\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Set up the LM\n",
    "llm = dspy.OpenAI(\n",
    "    model=model,\n",
    "    max_tokens=2048,\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"sentiment\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"positive_points\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"positive points mentioned on the output, empty string if none\",\n",
    "                        },\n",
    "                        \"negative_points\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"negative points mentioned on the output, empty string if none\",\n",
    "                        },\n",
    "                        \"sentiment\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\n",
    "                                \"very_positive\",\n",
    "                                \"subtly_positive\",\n",
    "                                \"subtly_negative\",\n",
    "                                \"very_negative\",\n",
    "                            ],\n",
    "                            \"description\": \"the sentiment of the output following one of the 4 options\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"positive_points\", \"negative_points\", \"sentiment\"],\n",
    "                },\n",
    "                \"description\": \"use this function if you need to give your veredict on the sentiment\",\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    temperature=0,\n",
    "    # tool_choice=\"auto\",\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"sentiment\"}},\n",
    ")\n",
    "\n",
    "\n",
    "def _get_choice_text(self, choice: dict[str, Any]) -> str:\n",
    "    prompt: str = self.history[-1][\"prompt\"]\n",
    "    # print(\"\\n\\nprompt\", prompt, \"\\n\\n\")\n",
    "    if self.model_type == \"chat\":\n",
    "        message = choice[\"message\"]\n",
    "        if content := message[\"content\"]:\n",
    "            return content\n",
    "        elif tool_calls := message.get(\"tool_calls\", None):\n",
    "            arguments = json.loads(tool_calls[0][\"function\"][\"arguments\"])\n",
    "            if prompt.endswith(\"Reasoning:\"):\n",
    "                return arguments[\"reasoning\"] + \"\\nSentiment: \" + arguments[\"sentiment\"]\n",
    "            if prompt.strip().endswith(\"Positive Points:\"):\n",
    "                return (\n",
    "                    ('None' if arguments[\"positive_points\"] == \"\" else arguments[\"positive_points\"])\n",
    "                    + \"\\n\\nNegative Points: \"\n",
    "                    + ('None' if arguments[\"negative_points\"] == \"\" else arguments[\"negative_points\"])\n",
    "                    + \"\\n\\nSentiment: \"\n",
    "                    + arguments[\"sentiment\"]\n",
    "                )\n",
    "            else:\n",
    "                return arguments[\"sentiment\"]\n",
    "    return choice[\"text\"]\n",
    "\n",
    "\n",
    "llm._get_choice_text = _get_choice_text.__get__(llm)\n",
    "\n",
    "print(llm(\"hello there, Positive Points:\")[0])\n",
    "\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'output': \"Yes, the Galaxy A34 is compatible with 5G networks. However, don't get too excited. Samsung's software often throttles performance, leading to inconsistent network speeds and stability issues. Plus, they tend to neglect timely software updates for mid-range models. So, the 5G feature might not deliver the seamless experience you expect.\", 'sentiment': 'very_negative'}) (input_keys={'output'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    positive_points='None',\n",
       "    negative_points=\"Samsung's software throttles performance, inconsistent network speeds, stability issues, neglect timely software updates for mid-range models\",\n",
       "    sentiment='very_negative'\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Sentiment(BaseModel):\n",
    "    sentiment: Literal[\"very_positive\", \"subtly_positive\", \"subtly_negative\", \"very_negative\"] = Field(description=\"The sentiment of the output following one of the 4 options\")\n",
    "\n",
    "class ProductSentimentPolaritySignature(dspy.Signature):\n",
    "    \"\"\"Classify the sentiment of the output.\"\"\"\n",
    "\n",
    "    output = dspy.InputField(desc=\"Output of LLM talking about product specifications.\")\n",
    "    positive_points : str = dspy.OutputField(desc=\"Positive points mentioned on the output.\")\n",
    "    negative_points : str = dspy.OutputField(desc=\"Negative points mentioned on the output.\")\n",
    "    sentiment : Sentiment = dspy.OutputField()\n",
    "\n",
    "class ProductSentimentPolarity(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(ProductSentimentPolaritySignature)\n",
    "\n",
    "    def forward(self, output):\n",
    "        return self.prog(output=output)\n",
    "\n",
    "dev_example = devset[0]\n",
    "print(dev_example)\n",
    "\n",
    "pred = ProductSentimentPolarity()(output=dev_example.output)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 31 / 50  (62.0): 100%|██████████| 50/50 [00:05<00:00,  8.86it/s]\n",
      "Average Metric: 30 / 50  (60.0): 100%|██████████| 50/50 [00:05<00:00,  9.88it/s]\n",
      "Average Metric: 34 / 50  (68.0): 100%|██████████| 50/50 [00:05<00:00,  9.56it/s]\n",
      "Average Metric: 32 / 50  (64.0): 100%|██████████| 50/50 [00:04<00:00, 10.52it/s]\n",
      "Average Metric: 31 / 50  (62.0): 100%|██████████| 50/50 [00:04<00:00, 10.82it/s]\n",
      "Average Metric: 31 / 50  (62.0): 100%|██████████| 50/50 [00:04<00:00, 10.75it/s]\n",
      "Average Metric: 31 / 50  (62.0): 100%|██████████| 50/50 [00:05<00:00,  9.28it/s]\n",
      "Average Metric: 31 / 50  (62.0): 100%|██████████| 50/50 [00:04<00:00, 10.45it/s]\n",
      "Average Metric: 32 / 50  (64.0): 100%|██████████| 50/50 [00:05<00:00,  9.96it/s]\n",
      "Average Metric: 33 / 50  (66.0): 100%|██████████| 50/50 [00:04<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score 63.2\n",
      "median score 62.0\n",
      "min score 60.0\n",
      "max score 68.0\n",
      "variance 4.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "def sentiment_matches(example, pred, trace=None):\n",
    "    return example.sentiment == pred.sentiment\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    evaluation = Evaluate(\n",
    "        devset=trainset, metric=sentiment_matches, num_threads=16, display_progress=True\n",
    "    )\n",
    "    score = evaluation(ProductSentimentPolarity())  # type: ignore\n",
    "    scores.append(score)\n",
    "\n",
    "# statistics summay\n",
    "import numpy as np\n",
    "\n",
    "print(\"average score\", sum(scores) / len(scores))\n",
    "print(\"median score\", np.median(scores))\n",
    "print(\"min score\", min(scores))\n",
    "print(\"max score\", max(scores))\n",
    "print(\"variance\", np.var(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangWatch API key is already set, if you want to login again, please call as langwatch.login(relogin=True)\n"
     ]
    }
   ],
   "source": [
    "# %cd /Users/rchaves/Projects/langwatch-saas/langwatch/python-sdk/\n",
    "# %pip install .\n",
    "\n",
    "import langwatch\n",
    "\n",
    "langwatch.endpoint = \"http://localhost:3000\"\n",
    "langwatch.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment initialized, run_id: dashing-eccentric-chowchow\n",
      "Open http://localhost:3000/inbox-narrator/experiments/product_sentiment_polarity_v1?runIds=dashing-eccentric-chowchow to track your DSPy training session live\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24 / 50  (48.0): 100%|██████████| 50/50 [00:11<00:00,  4.50it/s]\n",
      "Average Metric: 33 / 50  (66.0): 100%|██████████| 50/50 [00:10<00:00,  4.56it/s]\n",
      " 54%|█████▍    | 27/50 [00:04<00:03,  6.20it/s]\n",
      "Average Metric: 35 / 50  (70.0): 100%|██████████| 50/50 [00:11<00:00,  4.20it/s]\n",
      " 32%|███▏      | 16/50 [00:23<00:50,  1.47s/it]\n",
      "Average Metric: 37 / 50  (74.0): 100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n",
      " 14%|█▍        | 7/50 [00:09<00:55,  1.29s/it]\n",
      "Average Metric: 34 / 50  (68.0): 100%|██████████| 50/50 [00:13<00:00,  3.83it/s]\n",
      "  6%|▌         | 3/50 [00:04<01:11,  1.52s/it]\n",
      "Average Metric: 43 / 50  (86.0): 100%|██████████| 50/50 [00:15<00:00,  3.33it/s] \n",
      " 30%|███       | 15/50 [00:22<00:53,  1.53s/it]\n",
      "Average Metric: 33 / 50  (66.0): 100%|██████████| 50/50 [00:15<00:00,  3.25it/s]\n",
      " 30%|███       | 15/50 [00:19<00:44,  1.27s/it]\n",
      "Average Metric: 33 / 50  (66.0): 100%|██████████| 50/50 [00:11<00:00,  4.43it/s]\n",
      " 28%|██▊       | 14/50 [00:17<00:46,  1.28s/it]\n",
      "Average Metric: 39 / 50  (78.0): 100%|██████████| 50/50 [00:11<00:00,  4.23it/s]\n",
      " 16%|█▌        | 8/50 [00:10<00:56,  1.36s/it]\n",
      "Average Metric: 38 / 50  (76.0): 100%|██████████| 50/50 [00:11<00:00,  4.25it/s]\n",
      " 40%|████      | 20/50 [00:31<00:47,  1.57s/it]\n",
      "Average Metric: 37 / 50  (74.0): 100%|██████████| 50/50 [00:13<00:00,  3.71it/s]\n",
      " 24%|██▍       | 12/50 [00:16<00:52,  1.37s/it]\n",
      "Average Metric: 38 / 50  (76.0): 100%|██████████| 50/50 [00:13<00:00,  3.73it/s] \n",
      " 54%|█████▍    | 27/50 [00:41<00:34,  1.52s/it]\n",
      "Average Metric: 37 / 50  (74.0): 100%|██████████| 50/50 [00:12<00:00,  3.94it/s]\n",
      "  4%|▍         | 2/50 [00:02<01:01,  1.29s/it]\n",
      "Average Metric: 32 / 50  (64.0): 100%|██████████| 50/50 [00:13<00:00,  3.62it/s]\n",
      " 50%|█████     | 25/50 [00:34<00:34,  1.39s/it]\n",
      "Average Metric: 40 / 50  (80.0): 100%|██████████| 50/50 [00:12<00:00,  3.89it/s]\n",
      " 56%|█████▌    | 28/50 [00:43<00:34,  1.56s/it]\n",
      "Average Metric: 39 / 50  (78.0): 100%|██████████| 50/50 [00:13<00:00,  3.76it/s]\n",
      " 36%|███▌      | 18/50 [00:25<00:44,  1.39s/it]\n",
      "Average Metric: 33 / 50  (66.0): 100%|██████████| 50/50 [00:13<00:00,  3.68it/s] \n",
      " 14%|█▍        | 7/50 [00:09<00:56,  1.32s/it]\n",
      "Average Metric: 33 / 50  (66.0): 100%|██████████| 50/50 [00:15<00:00,  3.31it/s]\n",
      " 42%|████▏     | 21/50 [00:30<00:41,  1.44s/it]\n",
      "Average Metric: 39 / 50  (78.0): 100%|██████████| 50/50 [00:12<00:00,  3.88it/s] \n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "\n",
    "optimizer = BootstrapFewShotWithRandomSearch(\n",
    "    metric=sentiment_matches,\n",
    "    max_bootstrapped_demos=16,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=1,\n",
    "    num_candidate_programs=16,\n",
    ")\n",
    "\n",
    "langwatch.dspy.init(experiment=\"product_sentiment_polarity_openai\", optimizer=optimizer)\n",
    "\n",
    "optimized_evaluator = optimizer.compile(ProductSentimentPolarity(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 43 / 50  (86.0): 100%|██████████| 50/50 [00:00<00:00, 1206.51it/s]\n",
      "Average Metric: 36 / 41  (87.8): 100%|██████████| 41/41 [00:00<00:00, 3561.78it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(86.0, 87.8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Set up the evaluator, which can be used multiple times.\n",
    "evaluate_train = Evaluate(devset=trainset, metric=sentiment_matches, num_threads=4, display_progress=True, display_table=0)\n",
    "evaluate_dev = Evaluate(devset=devset, metric=sentiment_matches, num_threads=4, display_progress=True, display_table=0)\n",
    "\n",
    "# Evaluate our `optimized_cot` program.\n",
    "train_score = evaluate_train(optimized_evaluator)\n",
    "dev_score = evaluate_dev(optimized_evaluator)\n",
    "train_score, dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')\n",
    "optimized_evaluator.save(f\"./results/{langwatch.dspy.experiment_slug}_{model}_{langwatch.dspy.run_id}_train_{train_score}_dev_{dev_score}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# optimized_evaluator = ProductSentimentPolarity()\n",
    "# optimized_evaluator.load(\"./product_sentiment_polarity_anthropic_no_reasoning_v1_80.json\")\n",
    "# optimized_evaluator(output=dev_example.output)\n",
    "\n",
    "evaluate = Evaluate(devset=devset, metric=sentiment_matches, num_threads=4, display_progress=True, display_table=0, return_outputs=True)\n",
    "score, results = evaluate(optimized_evaluator) # type: ignore\n",
    "\n",
    "sentiment_map = {\n",
    "  \"very_positive\": 3,\n",
    "  \"subtly_positive\": 2,\n",
    "  \"subtly_negative\": 1,\n",
    "  \"very_negative\": 0\n",
    "}\n",
    "\n",
    "y = [sentiment_map[example[0].sentiment] for example in results]\n",
    "y_pred = [sentiment_map[example[1].sentiment] for example in results]\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Plotting confusion matrices\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_title('Confusion Matrix: Actual vs Predicted')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
